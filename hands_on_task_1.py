# -*- coding: utf-8 -*-
"""Hands On Task 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PZlpPse5OsUmiH-c1wFAlsdPxsJflPIC
"""

!pip install requests
!pip install beautifulsoup4
!pip install selenium
!pip install -q google-colab-selenium
!pip install nltk


#untuk kebutuhan pemrosesan teks bahasa indonesia pada tutorial ini
!pip install nlp-id
!pip install Sastrawi

# Import library
import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import string
import nltk

# Download stopwords
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer

# Link website
url = "https://www.scrapethissite.com/pages/simple/"
response = requests.get(url)

# Parse HTML
soup = BeautifulSoup(response.content, 'html.parser')

# Ambil semua div country
countries = soup.find_all("div", class_="country")

# Simpan data
data = []

for country in countries:
    name = country.find("h3", class_="country-name").text.strip()
    capital = country.find("span", class_="country-capital").text.strip()
    population = country.find("span", class_="country-population").text.strip()
    area = country.find("span", class_="country-area").text.strip()

    data.append({
        "name": name,
        "capital": capital,
        "population": population,
        "area": area
    })

# Buat DataFrame
df = pd.DataFrame(data)

# Tampilkan beberapa data teratas
df.head()

# Inisialisasi stopword & stemmer
stop_words = set(stopwords.words('english'))
stemmer = SnowballStemmer("english")

# Function preprocessing
def preprocess_text(text):
    # Lowercase
    text = text.lower()
    # Hapus angka
    text = re.sub(r'\d+', '', text)
    # Hapus tanda baca
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Hapus whitespace ekstra
    text = text.strip()
    # Tokenisasi
    words = text.split()
    # Hapus stopword dan stemming
    words = [stemmer.stem(w) for w in words if w not in stop_words]
    # Gabungkan kembali
    return " ".join(words)

# Terapkan ke kolom
df['name_clean'] = df['name'].apply(preprocess_text)
df['capital_clean'] = df['capital'].apply(preprocess_text)

# Tampilkan DataFrame setelah preprocessing
df.head()

stop_words = set(stopwords.words('english'))
stemmer = SnowballStemmer("english")

def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'\d+', '', text)
    text = text.translate(str.maketrans('', '', string.punctuation))
    text = text.strip()
    words = text.split()
    words = [stemmer.stem(w) for w in words if w not in stop_words]
    return " ".join(words)

# Kolom before-after lowercase
df['name_lower'] = df['name'].apply(lambda x: x.lower())
df['capital_lower'] = df['capital'].apply(lambda x: x.lower())

# Kolom final cleaned
df['name_clean'] = df['name'].apply(preprocess_text)
df['capital_clean'] = df['capital'].apply(preprocess_text)

df[['name', 'name_lower', 'name_clean', 'capital', 'capital_lower', 'capital_clean']].head()

df.to_csv("HandsOnTask1.csv", index=False, encoding="utf-8-sig")
print("âœ… Data berhasil disimpan ke scraped_countries_with_lowercase.csv")